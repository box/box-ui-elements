{"version":3,"file":"AiLlmEndpointParamsOpenAi.js","names":[],"sources":["../../../src/api/schemas/AiLlmEndpointParamsOpenAi.js"],"sourcesContent":["/**\n * @flow\n * @author Box\n */\n\nexport type AiLlmEndpointParamsOpenAiTypeField = 'openai_params';\n\nexport interface AiLlmEndpointParamsOpenAi {\n    /**\n     * The type of the AI LLM endpoint params object for OpenAI.\n     */\n    +type: AiLlmEndpointParamsOpenAiTypeField;\n    /**\n     * What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random,\n    while lower values like 0.2 will make it more focused and deterministic.\n    We generally recommend altering this or `top_p` but not both.\n     */\n    +temperature?: number;\n    /**\n     * An alternative to sampling with temperature, called nucleus sampling, where the model considers the results\n    of the tokens with `top_p` probability mass. So 0.1 means only the tokens comprising the top 10% probability\n    mass are considered. We generally recommend altering this or temperature but not both.\n     */\n    +top_p?: number;\n    /**\n     * Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the\n    text so far, decreasing the model's likelihood to repeat the same line verbatim.\n     */\n    +frequency_penalty?: number;\n    /**\n     * Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far,\n    increasing the model's likelihood to talk about new topics.\n     */\n    +presence_penalty?: number;\n    /**\n     * Up to 4 sequences where the API will stop generating further tokens.\n     */\n    +stop?: string;\n}\n"],"mappings":"","ignoreList":[]}