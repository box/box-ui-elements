{"version":3,"file":"AiLlmEndpointParamsGoogle.js","names":[],"sources":["../../../src/api/schemas/AiLlmEndpointParamsGoogle.js"],"sourcesContent":["/**\n * @flow\n * @author Box\n */\n\nexport type AiLlmEndpointParamsGoogleTypeField = 'google_params';\n\nexport interface AiLlmEndpointParamsGoogle {\n    /**\n     * The type of the AI LLM endpoint params object for Google.\n     */\n    +type: AiLlmEndpointParamsGoogleTypeField;\n    /**\n     * The temperature is used for sampling during response generation, which occurs when `top-P` and `top-K` are applied.\n    Temperature controls the degree of randomness in token selection.\n     */\n    +temperature?: number;\n    /**\n     * Top-P changes how the model selects tokens for output. Tokens are selected from the most (see `top-K`) to least probable\n    until the sum of their probabilities equals the `top-P` value.\n     */\n    +top_p?: number;\n    /**\n     * Top-K changes how the model selects tokens for output. A top-K of 1 means the next selected token is the\n    most probable among all tokens in the model's vocabulary (also called greedy decoding),\n    while a top-K of 3 means that the next token is selected from among the three most probable tokens by using temperature.\n     */\n    +top_k?: number;\n}\n"],"mappings":"","ignoreList":[]}